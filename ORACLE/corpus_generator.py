from pathlib import Path
import random

# Parameters
total_messages = 25000
english_ratio = 0.6
french_ratio = 0.2
chinese_ratio = 0.2

# Predefined messages for each language
english_samples = [
    "hey what's up? ğŸ˜„", "not much, just chilling", "same here, bored lol", "wanna hang out later?",
    "iâ€™m so tired today ğŸ˜©", "what time are we meeting?", "thatâ€™s awesome!", "can't believe that happened ğŸ¤¯",
    "bring snacks please ğŸ¿", "see you soon ğŸ‘‹", "yo that movie was wild ğŸ˜³", "i'll call you later ğŸ“",
    "did you eat?", "i'm outside", "let's go!", "are you home?", "what did you eat? ğŸ˜‹",
    "just woke up ğŸ˜´", "on my way ğŸš—", "hurry up!", "itâ€™s raining again ğŸŒ§ï¸", "iâ€™m at the shop ğŸ›ï¸",
    "guess what happened today", "lol you're crazy ğŸ˜‚", "send the pic!", "i miss summer already ğŸŒ",
    "donâ€™t forget to bring your charger ğŸ”Œ", "we need to talk", "iâ€™m free after 6", "text me when you're close",
    "omg no way ğŸ˜±", "we should do this again", "iâ€™ll be late", "good night ğŸŒ™", "morning! â˜€ï¸",
    "you up?", "same old same old", "you okay?", "i feel sick ğŸ¤’", "lmk if you need anything",
    "stop ghosting me ğŸ‘»", "this week flew by", "what show are you watching?", "need coffee asap â˜•",
    "canâ€™t stop laughing ğŸ˜‚", "talk later?", "whatâ€™s the plan for tomorrow?", "craving pizza ğŸ•",
    "did you know AI can write poems now? ğŸ¤–", "chatbots are getting scary good",
    "GPT stands for Generative Pre-trained Transformer", "AI doesnâ€™t actually think, it just predicts",
    "my phone knows what iâ€™m gonna type before i do ğŸ˜…", "they trained it on billions of words",
    "deepfakes are getting too real ğŸ˜³", "AI can generate music too ğŸµ",
    "have you seen those AI art things?", "i used an AI to clean up old photos!",
    "LLMs canâ€™t browse the web unless connected to one", "AI still makes up stuff sometimes tho",
    "data is the new oil apparently", "even spam filters use AI now",
    "face recognition uses machine learning ğŸ“·", "AI can spot diseases better than doctors in some cases",
    "i asked ChatGPT to write my essay ğŸ˜¬", "algorithms are everywhere",
    "your phone camera uses AI to enhance pics", "self-driving cars are basically AI on wheels ğŸš—",
    "neural networks are inspired by the brain", "AI can help detect fraud in banking",
    "AI learns from patterns in huge data sets", "some AIs can even code ğŸ˜²",
    "AI canâ€™t feel emotions but it can simulate them", "training AI models uses a lot of electricity âš¡"
]


french_samples = [
    "salut, Ã§a va ? ğŸ˜Š", "je suis fatiguÃ© aujourd'hui ğŸ˜´", "tu veux sortir ce soir ?",
    "je regarde un film maintenant ğŸ¬", "tâ€™as mangÃ© ?", "je peux pas ce soir ğŸ˜•", "viens chez moi ğŸ ",
    "c'Ã©tait trop bien !", "jâ€™ai trop mangÃ© ğŸ˜‚", "Ã  quelle heure ?", "trop cool !",
    "tu veux boire quelque chose ?", "on y va ?", "jâ€™arrive", "Ã§a me va ! ğŸ‘",
    "tu savais que l'IA peut Ã©crire des chansons maintenant ? ğŸ¶",
    "GPT veut dire Generative Pre-trained Transformer",
    "lâ€™IA ne pense pas, elle prÃ©dit",
    "jâ€™ai testÃ© une IA qui fait des dessins trop stylÃ©s ğŸ–¼ï¸",
    "mÃªme nos tÃ©lÃ©phones utilisent de lâ€™IA maintenant",
    "jâ€™ai vu une vidÃ©o deepfake, câ€™Ã©tait flippant ğŸ˜³",
    "les voitures autonomes utilisent lâ€™IA ğŸš—",
    "ton appareil photo amÃ©liore les photos avec de lâ€™IA",
    "ils entraÃ®nent les modÃ¨les sur des milliards de mots !",
    "lâ€™IA peut parfois inventer des trucs faux",
    "les filtres anti-spam utilisent aussi de lâ€™IA",
    "lâ€™IA peut aider Ã  dÃ©tecter le cancer ğŸ§¬",
    "je crois que mon appli mÃ©tÃ©o est plus intelligente que moi ğŸ˜‚",
    "les rÃ©seaux de neurones imitent le cerveau humain ğŸ§ ",
    "les assistants vocaux, câ€™est aussi de lâ€™IA",
    "jâ€™ai utilisÃ© une IA pour rÃ©sumer un article",
    "les algorithmes sont partout maintenant",
    "certaines IA peuvent gÃ©nÃ©rer du code",
    "les chatbots rÃ©pondent presque comme des humains maintenant",
    "lâ€™IA, câ€™est un peu la magie moderne âœ¨"
]

chinese_samples = [
    "ä½ çŸ¥é“å—ï¼Ÿç°åœ¨AIèƒ½å†™å°è¯´äº† ğŸ¤–", "GPTæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€ç§",
    "AIå…¶å®ä¸ä¼šæ€è€ƒï¼Œå®ƒåªæ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯", "æˆ‘çœ‹åˆ°ä¸€ä¸ªAIç”»çš„ç”»ï¼Œè¶…ç¾ ğŸ¨",
    "ç°åœ¨çš„æ‰‹æœºæ‹ç…§ä¹Ÿç”¨AIå¤„ç†", "æˆ‘ç”¨AIåšäº†ä¸ªå¤´åƒ ğŸ˜",
    "è‡ªåŠ¨é©¾é©¶ä¹Ÿæ˜¯AIçš„ä¸€ç§", "å¾ˆå¤šAPPæ¨èéƒ½æ˜¯ç®—æ³•åœ¨æ§åˆ¶",
    "AIè¿˜èƒ½å¸®åŠ©åŒ»ç”Ÿè¯Šæ–­ç–¾ç—… ğŸ§¬", "æœ‰äº›AIç”šè‡³èƒ½è‡ªå·±ç¼–ç¨‹äº† ğŸ˜¯",
    "AIæ˜¯é€šè¿‡å­¦ä¹ æµ·é‡æ•°æ®æ¥å·¥ä½œçš„", "AIä¹Ÿä¼šçŠ¯é”™ï¼Œåˆ«å¤ªä¿¡ ğŸ˜‚",
    "æœ‰ä¸ªAIå¸®æˆ‘å†™ä½œä¸šäº†", "ä½ å¬è¿‡AIå”±æ­Œå—ï¼Ÿ",
    "AIç”»çš„å›¾è·Ÿäººç”»çš„ä¸€æ ·", "AIæ¨¡å‹è®­ç»ƒçœŸçš„å¾ˆè€—ç”µ âš¡",
    "ç°åœ¨è¿ç¿»è¯‘éƒ½å¯ä»¥é AI", "AIä¼šæ¨¡ä»¿äººç±»çš„è¯´è¯æ–¹å¼",
    "åˆ·è§†é¢‘çš„æ¨èå…¶å®æ˜¯ç®—æ³•æ¨çš„", "AIèƒ½è¯†åˆ«ä½ çš„è„¸ ğŸ§ ",
    "æˆ‘åˆšæ´—å®Œæ¾¡ ğŸ›", "ä½ å»å“ªäº†ï¼Ÿ", "æˆ‘é¥¿æ­»äº† ğŸ˜©", "è¿™ä¸ªçœŸçš„å¤ªéš¾äº†",
    "æˆ‘ä»Šå¤©æ²¡è¯¾", "ä»Šå¤©è¶…çº§å¿™ ğŸ˜®â€ğŸ’¨", "ä½ æ€ä¹ˆè¿˜æ²¡åˆ°ï¼Ÿ", "æˆ‘æƒ³ç¡è§‰ ğŸ’¤",
    "æˆ‘ä»¬åƒç«é”…å§ ğŸ”¥", "è¿™ä¸ªä½ ä¸€å®šè¦çœ‹ï¼", "æˆ‘åœ¨çœ‹å‰§ ğŸ¬", "ä½ è¦ä¸è¦ä¸€èµ·æ¥ï¼Ÿ",
    "æˆ‘åœ¨æ’é˜Ÿ", "å¬è¯´è¦ä¸‹é›¨äº† ğŸŒ§ï¸", "åˆšæ‰å µè½¦äº†", "è¿™ä¸ªè¡¨æƒ…åŒ…å¤ªå¥½ç¬‘äº† ğŸ˜‚",
    "å‘¨æœ«æœ‰ç©ºå—ï¼Ÿ", "æˆ‘å®Œå…¨æ²¡å¬æ‡‚", "ä½ çŒœæˆ‘åˆšçœ‹åˆ°è°ï¼Ÿ", "å¤ªæ™šäº†æˆ‘è¦ç¡äº† ğŸ˜´",
    "å¥½åƒæ„Ÿå†’äº† ğŸ¤§", "ç­‰ä¸€ä¸‹æˆ‘æ´—ä¸ªè„¸", "æˆ‘å¦ˆå«æˆ‘åƒé¥­äº† ğŸš", "æˆ‘åœ¨æ‰“æ¸¸æˆ ğŸ®",
    "æ‰‹æœºå¿«æ²¡ç”µäº† ğŸ”‹", "åˆšåˆšç¡è¿‡å¤´äº† ğŸ˜…", "ä½ è§‰å¾—è¿™ä¸ªæ€ä¹ˆæ ·ï¼Ÿ", "æƒ³ä¸èµ·æ¥äº†",
    "ä½ æœ‰æ²¡æœ‰ç©ºé™ªæˆ‘èµ°èµ°ï¼Ÿ", "æˆ‘å·²ç»ä¸‹ç­äº†", "åˆ«å¤ªæƒ³æˆ‘å“¦ ğŸ˜œ", "ä½ ç°åœ¨æ–¹ä¾¿è¯´è¯å—ï¼Ÿ",
    "æˆ‘ä»¬æ˜å¤©å‡ ç‚¹è§ï¼Ÿ", "ä»Šå¤©çœŸæ˜¯å¥‡æ€ªçš„ä¸€å¤©", "æ™šç‚¹è§ï¼", "æˆ‘è¦å‡†å¤‡å‡ºé—¨äº† ğŸ‘Ÿ"
]

# Generate dataset
dataset = []
for _ in range(total_messages):
    lang = random.choices(
        population=["en", "fr", "zh"],
        weights=[english_ratio, french_ratio, chinese_ratio],
        k=1
    )[0]

    if lang == "en":
        msg = random.choice(english_samples)
    elif lang == "fr":
        msg = random.choice(french_samples)
    else:
        msg = random.choice(chinese_samples)

    dataset.append(msg)

# Join and save to file
dataset_text = "\n".join(dataset)
file_path = Path("corpus data/training_corpus.txt")
file_path.write_text(dataset_text, encoding="utf-8")
